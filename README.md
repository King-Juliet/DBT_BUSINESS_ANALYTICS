# BUILDING ROBUST DATA PIPELINE WITH DBT AND APACHEÂ AIRFLOW

# Project Description:
This project leverages dbt, a data transformation tool to ensure reliable data transformations in the datawarehouse and also documentation of the data artificats in the documentation site generated and hosted on AWS s3 bucket. Apache airflow was used to automate the entire workflow from extracting and load the data from on-prem OLTP database to the OLAP datawarehouse, and data modeling with transformation on the data warehouse.

# Project Requirements and Dependencies:
Python 

IDE -- VSCode

docker

Apache Airflow installed via docker-compose.yaml file

dbt adapter -- for this project, dbt-postgres

Database and datawarehouse of choice -- for this project, Postgres database

AWS account and access keys

s3 bucket with public access enabled

# How To Make Use Of The Project

Link to medium article: 
